# PS summary
                Shared Data    Consistency  Fault Tolerance
Graphlab [34]   graph          eventual     checkpoint
Petuum [12]     hash table     delay bound  none
REEF [10]       array          BSP          checkpoint
Naiad [37]      (key,value)    multiple     checkpoint
Mlbase [29]     table          BSP          RDD
PS              vector/matrix  various      continuous 

# Eric Xing
 # More effective distributed ml via a stale synchronous parallel parameter server
  NIPS 2013
 slowest and fastest workers must be <= s clocks apart
 Theoretical Analysis
 Exp:
  1. LDA Topic Modeling (collapsed Gibbs Sampling), New York Times
  2. Matrix Factorization (NetFlix 480k-by-18k)
  3. LASSO (Synthetic)

  # Petuum: A framework for iterative-convergent distributed ml
   NIPS 2013

  # Petuum: A New Platform for Distributed Machine Learning on Big Data
   KDD 2015

# PS
 # Scaling Distributed Machine Learning with the Parameter Server
  OSDI 2014
  Mu Li, Alex Smola
  Exp: click prediction
  Architecture:
   1. Training data;
   2. Worker group: task scheduler + n x worker nodes
   3. Server group: server manager + n x server nodes
   4. resource manager:
 (Key, Value) Vectors:
   BLAS, LAPACK, ATLAS
 Range Push and Pull:
 User-defined Functions on Server
 Asynch Tasks and Dependency
 Flexible Consistency: Sequential, Eventual, Bounded Delay
 Implementation:
  Vector Clock
  Messages
  Consistent Hashing
  Replication and Consistency
  Server Management: Yarn Mesos
 Exp:
  Sparse Logistic Regression
  LDA

# Distributed Graphlab: A framework for machine learning and data mining in the cloud.
 GraphLab
 VLDB 2012
